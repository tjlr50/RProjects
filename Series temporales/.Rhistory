chisq.test(tab)
dfR= df2 %>% select(BRAIN_VOLUME,AGE,SEX)
modelo_1 <- lm(BRAIN_VOLUME ~ AGE, data = dfR)
modelo_2 <- lm(BRAIN_VOLUME ~ poly(AGE, 2), data = dfR)
modelo_3 <- lm(BRAIN_VOLUME ~ poly(AGE, 3), data = dfR)
modelo_4 <- lm(BRAIN_VOLUME ~ poly(AGE, 4), data = dfR)
modelo_5 <- lm(BRAIN_VOLUME ~ poly(AGE, 5), data = dfR)
anova(modelo_1, modelo_2, modelo_3, modelo_4, modelo_5)
f1= lm(dfR$BRAIN_VOLUME ~ dfR$AGE + dfR$SEX)
f2= lm(dfR$BRAIN_VOLUME ~ dfR$AGE + dfR$SEX + I(dfR$AGE^2))
f3= lm(dfR$BRAIN_VOLUME ~ dfR$AGE + dfR$SEX + I(dfR$AGE^2) + I(dfR$AGE^3))
f4= lm(dfR$BRAIN_VOLUME ~ dfR$AGE + dfR$SEX + I(dfR$AGE^2) + I(dfR$AGE^3) + I(dfR$AGE^4))
f5= lm(dfR$BRAIN_VOLUME ~ dfR$AGE + dfR$SEX + I(dfR$AGE^2) + I(dfR$AGE^3) + I(dfR$AGE^4) + I(dfR$AGE^5))
anova(f1,f2,f3,f4,f5)
summary(f2)
plot.new()
plot(fitted(f4), residuals(f4), xlab = "Approached values",  ylab = "Residuals")
abline(h=0, lty=2)
lines(smooth.spline(fitted(f4), residuals(f4)))
plot(f4$residuals)
summary(f4$residuals)
boxplot(f4$residuals)
#install.packages("caret")
library(caret)
modelLookup()
trainIndex <- createDataPartition(dfR$BRAIN_VOLUME,
p = .8,
list = FALSE,
times = 1)
b_train <- dfR[trainIndex, ]
b_test  <- dfR[-trainIndex, ]
fitControl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 1)
# Y esta función entrena el método
caret_lm_model <- train(BRAIN_VOLUME ~ .,
data = b_train,
method = "lm",
trControl = fitControl)
caret_lm_model
# Métodos que queremos ejecutar
methods <- c("knn", "gbm", "lm")
# Los ejecutamos usando lapply
methods %>%
lapply(function(x) {
train(BRAIN_VOLUME ~ .,
data = b_train,
method = x,
trControl = fitControl,
verbose = FALSE)
}) -> caret_results
# A cada elemento del resultado le ponemos nombre
names(caret_results) <- methods
#install.packages("kableExtra", dependencies = TRUE)
# Recorremos todos los métodos y tomamos el RMSE de cada uno
best_rmse <- sapply(caret_results, function(i) min(i$results$RMSE, na.rm = TRUE))
# El mejor será el que tenga un RMSE más bajo
best_regressor <- names(best_rmse)[which.min(best_rmse)]
# Los mostramos en forma de tabla
w <- best_rmse %>% as.data.frame()
colnames(w) <- "RMSE"
w %>%
knitr::kable(format = "html") %>% kableExtra::kable_styling(font_size = 14)
cat("El metodo que minimiza la medida del error:",best_regressor)
plot(caret_results$gbm)
dfRG= df2 %>% select(SEX,AGE,GM_VOLUME)
dfRW= df2 %>% select(SEX,AGE,WM_VOLUME)
modelo_1G <- lm(GM_VOLUME ~ AGE, data = dfRG)
modelo_2G <- lm(GM_VOLUME ~ poly(AGE, 2), data = dfRG)
modelo_3G <- lm(GM_VOLUME ~ poly(AGE, 3), data = dfRG)
modelo_4G <- lm(GM_VOLUME ~ poly(AGE, 4), data = dfRG)
modelo_5G <- lm(GM_VOLUME ~ poly(AGE, 5), data = dfRG)
anova(modelo_1G, modelo_2G, modelo_3G, modelo_4G, modelo_5)
f1G= lm(dfRG$GM_VOLUME ~ dfRG$AGE + dfRG$SEX)
f2G= lm(dfRG$GM_VOLUME ~ dfRG$AGE + dfRG$SEX + I(dfRG$AGE^2))
f3G= lm(dfRG$GM_VOLUME ~ dfRG$AGE + dfRG$SEX + I(dfRG$AGE^2) + I(dfRG$AGE^3))
f4G= lm(dfRG$GM_VOLUME ~ dfRG$AGE + dfRG$SEX + I(dfRG$AGE^2) + I(dfRG$AGE^3) + I(dfRG$AGE^4))
f5G= lm(dfRG$GM_VOLUME ~ dfRG$AGE + dfRG$SEX + I(dfRG$AGE^2) + I(dfRG$AGE^3) + I(dfRG$AGE^4) + I(dfRG$AGE^5))
anova(f1G,f2G,f3G,f4G,f5G)
summary(f4G)
plot.new()
plot(fitted(f4G), residuals(f4G), xlab = "Approached values",  ylab = "Residuals")
abline(h=0, lty=2)
lines(smooth.spline(fitted(f4G), residuals(f4G)))
plot(f4G$residuals)
summary(f4G$residuals)
boxplot(f4G$residuals)
trainIndex <- createDataPartition(dfRG$GM_VOLUME,
p = .8,
list = FALSE,
times = 1)
b_train <- dfRG[trainIndex, ]
b_test  <- dfRG[-trainIndex, ]
fitControl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 1)
# Y esta función entrena el método
caret_lm_model <- train(GM_VOLUME ~ .,
data = b_train,
method = "lm",
trControl = fitControl)
caret_lm_model
# Métodos que queremos ejecutar
methods <- c("knn", "gbm", "lm")
# Los ejecutamos usando lapply
methods %>%
lapply(function(x) {
train(GM_VOLUME ~ .,
data = b_train,
method = x,
trControl = fitControl,
verbose = FALSE)
}) -> caret_results
# A cada elemento del resultado le ponemos nombre
names(caret_results) <- methods
#install.packages("kableExtra", dependencies = TRUE)
# Recorremos todos los métodos y tomamos el RMSE de cada uno
best_rmse <- sapply(caret_results, function(i) min(i$results$RMSE, na.rm = TRUE))
# El mejor será el que tenga un RMSE más bajo
best_regressor <- names(best_rmse)[which.min(best_rmse)]
# Los mostramos en forma de tabla
w <- best_rmse %>% as.data.frame()
colnames(w) <- "RMSE"
w %>%
knitr::kable(format = "html") %>% kableExtra::kable_styling(font_size = 14)
cat("El metodo que minimiza la medida del error:",best_regressor)
plot(caret_results$gbm)
modelo_1G <- lm(GM_VOLUME ~ AGE, data = dfRG)
modelo_2G <- lm(GM_VOLUME ~ poly(AGE, 2), data = dfRG)
modelo_3G <- lm(GM_VOLUME ~ poly(AGE, 3), data = dfRG)
modelo_4G <- lm(GM_VOLUME ~ poly(AGE, 4), data = dfRG)
modelo_5G <- lm(GM_VOLUME ~ poly(AGE, 5), data = dfRG)
anova(modelo_1G, modelo_2G, modelo_3G, modelo_4G, modelo_5G)
f1G= lm(dfRG$GM_VOLUME ~ dfRG$AGE + dfRG$SEX)
f2G= lm(dfRG$GM_VOLUME ~ dfRG$AGE + dfRG$SEX + I(dfRG$AGE^2))
f3G= lm(dfRG$GM_VOLUME ~ dfRG$AGE + dfRG$SEX + I(dfRG$AGE^2) + I(dfRG$AGE^3))
f4G= lm(dfRG$GM_VOLUME ~ dfRG$AGE + dfRG$SEX + I(dfRG$AGE^2) + I(dfRG$AGE^3) + I(dfRG$AGE^4))
f5G= lm(dfRG$GM_VOLUME ~ dfRG$AGE + dfRG$SEX + I(dfRG$AGE^2) + I(dfRG$AGE^3) + I(dfRG$AGE^4) + I(dfRG$AGE^5))
anova(f1G,f2G,f3G,f4G,f5G)
summary(f4G)
plot.new()
plot(fitted(f4G), residuals(f4G), xlab = "Approached values",  ylab = "Residuals")
abline(h=0, lty=2)
lines(smooth.spline(fitted(f4G), residuals(f4G)))
plot(f4G$residuals)
summary(f4G$residuals)
boxplot(f4G$residuals)
trainIndex <- createDataPartition(dfRG$GM_VOLUME,
p = .8,
list = FALSE,
times = 1)
b_train <- dfRG[trainIndex, ]
b_test  <- dfRG[-trainIndex, ]
fitControl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 1)
# Y esta función entrena el método
caret_lm_model <- train(GM_VOLUME ~ .,
data = b_train,
method = "lm",
trControl = fitControl)
caret_lm_model
# Métodos que queremos ejecutar
methods <- c("knn", "gbm", "lm")
# Los ejecutamos usando lapply
methods %>%
lapply(function(x) {
train(GM_VOLUME ~ .,
data = b_train,
method = x,
trControl = fitControl,
verbose = FALSE)
}) -> caret_results
# A cada elemento del resultado le ponemos nombre
names(caret_results) <- methods
#install.packages("kableExtra", dependencies = TRUE)
# Recorremos todos los métodos y tomamos el RMSE de cada uno
best_rmse <- sapply(caret_results, function(i) min(i$results$RMSE, na.rm = TRUE))
# El mejor será el que tenga un RMSE más bajo
best_regressor <- names(best_rmse)[which.min(best_rmse)]
# Los mostramos en forma de tabla
w <- best_rmse %>% as.data.frame()
colnames(w) <- "RMSE"
w %>%
knitr::kable(format = "html") %>% kableExtra::kable_styling(font_size = 14)
cat("El metodo que minimiza la medida del error:",best_regressor)
plot(caret_results$gbm)
modelo_1W <- lm(WM_VOLUME ~ AGE, data = dfRW)
modelo_2W <- lm(WM_VOLUME ~ poly(AGE, 2), data = dfRW)
modelo_3W <- lm(WM_VOLUME ~ poly(AGE, 3), data = dfRW)
modelo_4W <- lm(WM_VOLUME ~ poly(AGE, 4), data = dfRW)
modelo_5W <- lm(WM_VOLUME ~ poly(AGE, 5), data = dfRW)
anova(modelo_1W, modelo_2W, modelo_3W, modelo_4W, modelo_5W)
f1W= lm(dfRW$WM_VOLUME ~ dfRW$AGE + dfRW$SEX)
f2W= lm(dfRW$WM_VOLUME ~ dfRW$AGE + dfRW$SEX + I(dfRW$AGE^2))
f3W= lm(dfRW$WM_VOLUME ~ dfRW$AGE + dfRW$SEX + I(dfRW$AGE^2) + I(dfRW$AGE^3))
f4W= lm(dfRW$WM_VOLUME ~ dfRW$AGE + dfRW$SEX + I(dfRW$AGE^2) + I(dfRW$AGE^3) + I(dfRW$AGE^4))
f5W= lm(dfRW$WM_VOLUME ~ dfRW$AGE + dfRW$SEX + I(dfRW$AGE^2) + I(dfRW$AGE^3) + I(dfRW$AGE^4) + I(dfRW$AGE^5))
anova(f1W,f2W,f3W,f4W,f5W)
summary(f4W)
plot.new()
plot(fitted(f4W), residuals(f4W), xlab = "Approached values",  ylab = "Residuals")
abline(h=0, lty=2)
lines(smooth.spline(fitted(f4W), residuals(f4W)))
plot(f4W$residuals)
summary(f4W$residuals)
boxplot(f4W$residuals)
trainIndex <- createDataPartition(dfRW$WM_VOLUME,
p = .8,
list = FALSE,
times = 1)
b_train <- dfRW[trainIndex, ]
b_test  <- dfRW[-trainIndex, ]
fitControl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 1)
# Y esta función entrena el método
caret_lm_model <- train(WM_VOLUME ~ .,
data = b_train,
method = "lm",
trControl = fitControl)
caret_lm_model
# Métodos que queremos ejecutar
methods <- c("knn", "gbm", "lm")
# Los ejecutamos usando lapply
methods %>%
lapply(function(x) {
train(WM_VOLUME ~ .,
data = b_train,
method = x,
trControl = fitControl,
verbose = FALSE)
}) -> caret_results
# A cada elemento del resultado le ponemos nombre
names(caret_results) <- methods
#install.packages("kableExtra", dependencies = TRUE)
# Recorremos todos los métodos y tomamos el RMSE de cada uno
best_rmse <- sapply(caret_results, function(i) min(i$results$RMSE, na.rm = TRUE))
# El mejor será el que tenga un RMSE más bajo
best_regressor <- names(best_rmse)[which.min(best_rmse)]
# Los mostramos en forma de tabla
w <- best_rmse %>% as.data.frame()
colnames(w) <- "RMSE"
w %>%
knitr::kable(format = "html") %>% kableExtra::kable_styling(font_size = 14)
cat("El metodo que minimiza la medida del error:",best_regressor)
plot(caret_results$gbm)
dfRA= df2 %>% select(WM_VOLUME, GM_VOLUME,AGE,SEX)
trainIndex <- createDataPartition(dfRA$AGE,
p = .8,
list = FALSE,
times = 1)
b_train <- dfRA[trainIndex, ]
b_test  <- dfRA[-trainIndex, ]
# Métodos que queremos ejecutar
methods <- c("knn", "gbm", "lm")
# Los ejecutamos usando lapply
methods %>%
lapply(function(x) {
train(AGE ~ .,
data = b_train,
method = x,
trControl = fitControl,
verbose = FALSE)
}) -> caret_results
# A cada elemento del resultado le ponemos nombre
names(caret_results) <- methods
#install.packages("kableExtra", dependencies = TRUE)
# Recorremos todos los métodos y tomamos el RMSE de cada uno
best_rmse <- sapply(caret_results, function(i) min(i$results$RMSE, na.rm = TRUE))
# El mejor será el que tenga un RMSE más bajo
best_regressor <- names(best_rmse)[which.min(best_rmse)]
# Los mostramos en forma de tabla
w <- best_rmse %>% as.data.frame()
colnames(w) <- "RMSE"
w %>%
knitr::kable(format = "html") %>% kableExtra::kable_styling(font_size = 14)
cat("El metodo que minimiza la medida del error:",best_regressor)
fitControl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 1)
# Y esta función entrena el método
caret_lm_model <- train(AGE ~ .,
data = b_train,
method = "gbm",
trControl = fitControl)
plot(caret_results$gbm)
plot.new()
plot(fitted(f2), residuals(f2), xlab = "Approached values",  ylab = "Residuals")
abline(h=0, lty=2)
lines(smooth.spline(fitted(f2), residuals(f2)))
plot(f2$residuals)
summary(f2$residuals)
boxplot(f2$residuals)
105.9449*105.9449
modelo_1G <- lm(GM_VOLUME ~ AGE, data = dfRG)
modelo_2G <- lm(GM_VOLUME ~ poly(AGE, 2), data = dfRG)
modelo_3G <- lm(GM_VOLUME ~ poly(AGE, 3), data = dfRG)
modelo_4G <- lm(GM_VOLUME ~ poly(AGE, 4), data = dfRG)
modelo_5G <- lm(GM_VOLUME ~ poly(AGE, 5), data = dfRG)
anova(modelo_1G, modelo_2G, modelo_3G, modelo_4G, modelo_5G)
summary(f2G)
plot.new()
plot(fitted(f2G), residuals(f2G), xlab = "Approached values",  ylab = "Residuals")
abline(h=0, lty=2)
lines(smooth.spline(fitted(f2G), residuals(f2G)))
plot(f2G$residuals)
summary(f2G$residuals)
boxplot(f2G$residuals)
56.17554*56.17554
# Métodos que queremos ejecutar
methods <- c("knn", "gbm", "lm")
# Los ejecutamos usando lapply
methods %>%
lapply(function(x) {
train(GM_VOLUME ~ .,
data = b_train,
method = x,
trControl = fitControl,
verbose = FALSE)
}) -> caret_results
# A cada elemento del resultado le ponemos nombre
names(caret_results) <- methods
summary(f2G)
library(readr)
library(tidyverse)
library(dplyr)
knit_with_parameters('C:/Users/TOMAS/Desktop/UMA/CienciaDatosl/Proyecto series temporales/Series/SeriesTemporales.Rmd', encoding = 'UTF-8')
tinytex::install_tinytex()
unlink('ProyectoContrastesRegresion_cache', recursive = TRUE)
summary(f1W)
plot.new()
plot(fitted(f1W), residuals(f1W), xlab = "Approached values",  ylab = "Residuals")
abline(h=0, lty=2)
lines(smooth.spline(fitted(f1W), residuals(f1W)))
plot(f1W$residuals)
summary(f1W$residuals)
boxplot(f1W$residuals)
fitControl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 1)
# Y esta función entrena el método
caret_lm_model <- train(AGE ~ .,
data = b_train,
method = "lm",
trControl = fitControl)
plot(caret_results$lm)
plot(caret_results$lm)
caret_results$lm
dfRL= df2 %>% select(BRAIN_VOLUME,WM_VOLUME, GM_VOLUME,AGE,SEX, CLASS)
trainIndex <- createDataPartition(dfRL$CLASS,
p = .8,
list = FALSE,
times = 1)
b_train <- dfRL[trainIndex, ]
b_test  <- dfRL[-trainIndex, ]
dfRL= df2 %>% select(BRAIN_VOLUME,WM_VOLUME, GM_VOLUME,AGE,SEX, CLASS)
trainIndex <- createDataPartition(dfRL$CLASS,
p = .8,
list = FALSE,
times = 1)
b_train <- dfRL[trainIndex, ]
b_test  <- dfRL[-trainIndex, ]
set.seed(123)
dfRL= df2 %>% select(BRAIN_VOLUME,WM_VOLUME, GM_VOLUME,AGE,SEX, CLASS)
set.seed(123)
trainIndex <- createDataPartition(dfRL$CLASS,
p = .8,
list = FALSE,
times = 1)
b_train <- dfRL[trainIndex, ]
b_test  <- dfRL[-trainIndex, ]
gm <- glm(formula = dfRL$CLASS ~ dfRL$BRAIN_VOLUME  + dfRL$WM_VOLUME + dfRL$GM_VOLUME + dfRL$AGE + dfRL$SEX, data = dfRL, family = binomial)
gm <- glm(formula = dfRL$CLASS ~ dfRL$BRAIN_VOLUME  + dfRL$WM_VOLUME + dfRL$GM_VOLUME + dfRL$AGE + dfRL$SEX, data = dfRL, family = binomial)
gm <- glm(formula = CLASS ~ BRAIN_VOLUME  + WM_VOLUME + GM_VOLUME + AGE + SEX, data = dfRL, family = binomial)
dfRL$CLASS = as.factor(dfRL$CLASS)
trainIndex <- createDataPartition(dfRL$CLASS,
p = .8,
list = FALSE,
times = 1)
b_train <- dfRL[trainIndex, ]
b_test  <- dfRL[-trainIndex, ]
```{r}
gm <- glm(formula = CLASS ~ BRAIN_VOLUME  + WM_VOLUME + GM_VOLUME + AGE + SEX, data = dfRL, family = binomial)
gm
summary(gm)
dfRL= df2 %>% select(BRAIN_VOLUME,WM_VOLUME, GM_VOLUME,AGE,SEX, CLASS)
set.seed(123)
dfRL$SEX = as.factor(dfRL$SEX)
dfRL$CLASS = as.factor(dfRL$CLASS)
trainIndex <- createDataPartition(dfRL$CLASS,
p = .8,
list = FALSE,
times = 1)
b_train <- dfRL[trainIndex, ]
b_test  <- dfRL[-trainIndex, ]
gm <- glm(formula = CLASS ~ BRAIN_VOLUME  + WM_VOLUME + GM_VOLUME + AGE + SEX, data = dfRL, family = binomial)
summary(gm)
gm <- glm(formula = CLASS ~ BRAIN_VOLUME  + WM_VOLUME + GM_VOLUME, data = dfRL, family = binomial)
summary(gm)
gm <- glm(formula = CLASS ~ BRAIN_VOLUME  + WM_VOLUME + GM_VOLUME + AGE + SEX, data = b_train, family = binomial)
summary(gm)
gm <- glm(formula = CLASS ~ BRAIN_VOLUME  + WM_VOLUME + GM_VOLUME, data = b_train, family = binomial)
summary(gm)
logit <- predict(gm,b_test)
logit <- predict(gm,b_test)
logit <- predict(gm,b_test)
p <- 1 / (1 + exp(-logit))
p
# Validación cruzada
fitControl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 1)
methods <- c("svmLinear", "rpart", "regLogistic", "rf")
# Se ejecutan
methods %>%
lapply(function(x) {
caret::train(Class ~ .,
data = dfRL,
method = x,
trControl = fitControl)
}) -> caret_class_results
methods <- c("svmLinear", "rpart", "regLogistic", "rf")
# Se ejecutan
methods %>%
lapply(function(x) {
caret::train(CLASS ~ .,
data = dfRL,
method = x,
trControl = fitControl)
}) -> caret_class_results
names(caret_class_results) <- methods
methods <- c("svmLinear", "rpart", "regLogistic", "rf")
# Se ejecutan
methods %>%
lapply(function(x) {
caret::train(CLASS ~ .,
data = b_train,
method = x,
trControl = fitControl)
}) -> caret_class_results
names(caret_class_results) <- methods
# Validación cruzada
fitControl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 1)
methods <- c("svmLinear", "rpart", "regLogistic", "rf")
# Se ejecutan
methods %>%
lapply(function(x) {
caret::train(CLASS ~ .,
data = b_train,
method = x,
trControl = fitControl)
}) -> caret_class_results
names(caret_class_results) <- methods
methods <- c("svmLinear", "rpart", "regLogistic", "C5.0")
# Se ejecutan
methods %>%
lapply(function(x) {
caret::train(CLASS ~ .,
data = b_train,
method = x,
trControl = fitControl)
}) -> caret_class_results
names(caret_class_results) <- methods
1
methods <- c("svmLinear", "rpart", "regLogistic", "C5.0")
# Se ejecutan
methods %>%
lapply(function(x) {
caret::train(CLASS ~ .,
data = b_train,
method = x,
trControl = fitControl)
}) -> caret_class_results
names(caret_class_results) <- methods
names(caret_class_results) <- methods
print(caret_class_results$svmLinear)
plot(caret_class_results$svmLinear)
print
kernlab::plot(caret_class_results$svmLinear)
kernlab::plot(caret_class_results$rpart)
kernlab::plot(caret_class_results$regLogistic)
names(caret_class_results) <- methods
print(caret_class_results$svmLinear)
folds <- createFolds(dfRL$CLASS, k = 10)
folds <- createFolds(dfRL$CLASS, k = 5)
# Recorremos todos los resultados, almacenando la métrica
# accuracy (la precisión)
best_acc <- sapply(caret_class_results, function(i) max(i$results$Accuracy, na.rm = TRUE))
best_classifier <- names(best_acc)[which.max(best_acc)]
w <- best_acc %>% as.data.frame()
colnames(w) <- "Accuracy"
w %>%
knitr::kable(format = "html") %>%
kableExtra::kable_styling(font_size = 14)
predicted_class <- caret_class_results[[best_classifier]] %>%
predict(b_test)
# Valores estimados como primer argumento, valores reales omo segundo
confusionMatrix(predicted_class, dfRL$CLASS)
# Valores estimados como primer argumento, valores reales omo segundo
confusionMatrix(predicted_class, b_test$CLASS)
plot(caret_class_results[[best_classifier]])
theta =c(0.07,0.08,0.09)
pTheta <- pmin(theta, 1 - theta)
pTheta
plot(theta, pTheta)
tinytex::install_tinytex()
unlink('ProyectoContrastesRegresion_cache', recursive = TRUE)
unlink('ProyectoContrastesRegresion_cache', recursive = TRUE)
knit_with_parameters('C:/Users/TOMAS/Desktop/UMA/CienciaDatosl/Proyecto - Aprendizaje supervisado con caret/Proyecto l/ProyectoContrastesRegresion.Rmd', encoding = 'UTF-8')
